# 🦁 Deep Learning repository 🦁


## ✅ 머신러닝과 딥러닝

* 머신러닝 
  * 인간이 직접 특징을 도출할 수 있게 설계하여 예측값 출력
* 딥러닝
  * 인공지능 스스로 일정 범주의 데이터를 바탕으로 공통된 특징 도출
  * 그 특징으로 예측값 출력

* 1950 년대 이전부터 신경망 연구
* 그렇지만 하드웨어의 한계와 데이터 부족으로 정체


<br>

## ✅ 순전파와 역전파
### - 순전파
* 인공 신경망에서 입력층에서 출력층 방향으로 예측값의 연산이 진행되는 과정
* 입력값은 입력층, 은닉층을 지남
* 각 층에서 가중치와 함께 연산되어, 출력층에서 모든 연산을 마친 예측값 출력 


### - 역전파
* 순전파와 반대로 출력층에서 입력층 방향으로 계산하면서 가중치 업데이트
* 역전파를 통해 가중치 비율을 조정하여 오차 감소 진행
* 다시 순전파 진행으로 오차 감소 확인

### - 활성화 함수
* 은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수
* 입력값들의 수학적 선형 결합을 다양한 형태의 `비선형` 결합으로 변환하는 역할


![활성화 함수 사진]("../../../img/AF.png) <br>

[출처](https://medium.com/@kmkgabia/ml-sigmoid-%EB%8C%80%EC%8B%A0-relu-%EC%83%81%ED%99%A9%EC%97%90-%EB%A7%9E%EB%8A%94-%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-c65f620ad6fd) <br>


<br>

### - 기울기 소실
* 깊은 인공 신경망을 학습할 때 역전파 과정에서 입력층으로 갈수록 기울기가 점차 작아지는 현상
* 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면 최적의 모델을 찾을 수 없음
* `ReLU` 함수
  * 기울기 소실을 완화하는 가장 간단한 방법
  * 미분 계산이 훨씬 간편해져 학습 속도 개선
  * 단점 : Dying ReLU (x가 0보다 작거나 같으면 항상 동일한 값인 0을 출력하기 때문에 발생)
* `Leaky ReLU`
  * Dying ReLU 현상을 해결하기 위해 나온 변형 함수
  * 입력값이 음수일 경우 0이 아니라 매우 작은 값 반환
  
### - 기울기 폭주
* 기울기 소실의 반대 경우로 기울기가 점차 커지더니 가중치들이 비정상적으로 커지는 것
* 순환 신경망(RNN)에서 쉽게 발생


- 사실(Fact): SMOTE를 이용한 오버샘플링 기법을 실습했고 딥러닝 이론을 배웠다.
- 느낌(Feeling): 새로운 단어와 코드로 정신이 없었다.
- 교훈(Finding): 못 외운다. 익숙해지고 공식문서 찾아보는 버릇하자. 