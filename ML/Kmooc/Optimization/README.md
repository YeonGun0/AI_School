# 🦁 TIL

## ✅ 최적화와 모형 학습
### - Optimization
* ML model 최적화
  * 손실 함수를 최소화하는 최적의 함수, 패턴, 모형을 찾는 것

<br>

## ✅ 경사하강법 개요
* 손실 함수의 값을 가장 빨리 줄일 수 있는, 접선의 기울기가 가장 큰 곳으로 최적화
* 기울기가 0이 되는 지점을 찾는 것이 목표
* 0이 되는 지점이 여러 개 있을 수 있음 


<br>

## ✅ 경사하강법 심화
### - Learning Rate(step size)
* learning rate 가 큰 경우 빠르게 최솟값을 향해 최적화될 수 있음
* 너무 크면 오히려 손실이 발산할 수 있음
* 그렇다고 너무 작게하면 최적화에 너무 오랜 시간 소요
> `적절한` learning rate 를 찾는 것이 중요

<br>

### - Stochastic Gradient Descent
* 딥러닝에서 많이 활용되는 방법
* 데이터 전체를 사용하면 많은 시간 소요 & 메모리 과부하
* 전체 데이터에서 일부를 추출하여 경사하강법 진행
  
<br>

* 데이터를 n개의 집단(batch)으로 분할
* 각 집단에 대해 경사하강법 n번 진행
* 결과가 완벽하진 않지만 대략적으로 빠르게 최적화할 수 있다는 장점


<br>

### - Global vs Local minimum
* `Local minimum`
  * 극소값
  * 극솟값의 경사도 기울기는 0
  * 극솟값에 도착하면 최적화가 멈추는 한계

* `Saddle point`
  * 극솟값은 아니나 경사도가 0이 되는 지점

* `Global minimum`
  * 최솟값
  * 극솟값 중 가장 작은 값

* `Momentum`(가속도, 관성)
  * 극솟값 or Saddle point 에서 최적화를 멈추지 않고 최솟값을 찾기 위해 필요한 요소
  * 내려왔던 방향과 가속도를 기억해뒀다가 극솟값에 왔을 때 더해줌으로써 경사를 넘어감
  * 딥러닝에서 가장 많이 이용되는 방식 : `SGD + Momentum`